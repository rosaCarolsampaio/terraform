image: ubuntu

stages:
  - lambda
  - kafka
  - golang
  - create
  - dynamodb
  - test
  - list


# services:
#   - name: docker:dind
#     alias: docker 
  
services:
  - name: localstack/localstack
    alias: localstack

variables:
  SERVICES: s3,sns,sqs, dynamodb,lambda, kafka, ec2
  HOSTNAME_EXTERNAL: localstack 
  AWS_DEFAULT_REGION: eu-west-2
  AWS_ACCESS_KEY_ID: localkey 
  AWS_SECRET_ACCESS_KEY: localsecret
  AWS_DYNAMODB_ENDPOINT: http://dynamodb:8000
  GIT_CLEAN_FLAGS: none
  LOCALSTACK_HOST: localstack
  HOST: docker:2375
  DEBUG: 1
  LAMBDA_REMOTE_DOCKER: "false"
  #DOCKER_DRIVER: overlay2
  


before_script:
 # - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.example.com
  - apt-get -y update
  - apt-get -y install python3-pip jq curl
  - pip3 install --upgrade awscli  awscli-local
    
cache:
  paths:
    - s3.localhost.localstack.cloud/


Lambda.AWS.local:
  stage: lambda
  script:
    # create role 
    #- awslocal iam create-role --role-name lambda-example --assume-role-policy-document '{"Version":"2012-10-28","Statement":[{ "Effect":"Allow", "Principal":{"Service":"lambda.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
    # copy the files and create trust 
   
     - awslocal s3 mb s3://lambda-bucket
     - awslocal s3api put-object --bucket lambda-bucket --key lambda
     - awslocal s3 cp ./src/ s3://lambda-bucket/lambda --recursive --acl public-read-write
     - curl -v http://localstack:4566/lambda-bucket | true
     - awslocal lambda create-function --function-name lambda-example  --code S3Bucket="lambda-bucket",S3Key="s3://lambda-bucket/lambda/" --runtime nodejs14.x  --role "xpto" 
     - awslocal iam create-role --role-name lambda-example --assume-role-policy-document file://src/script/trust-policy.json
    # adiciona a permission to role
   # - awslocal iam attach-role-policy --role-name lambda-example --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole


    # create the function- 

    # - awslocal lambda create-function --function-name  hello --zip-file handler.zip --handler "handler.hello" --runtime nodejs14.x --role "xpto" 
    # - awslocal lambda list-functions --region $DEFAULT_REGION
    # - awslocal --endpoint-url $AWS_ENDPOINT lambda  invoke --function-name "hello" 
     - awslocal  lambda list-functions


test-localstack:
  stage: list
  dependencies:
    - test-application
  script:
    - awslocal s3 ls
    - awslocal s3api list-buckets 

test-application:
  stage: test 
  # extends: build
  # cache:
  #   paths:
  #     - s3://my-test-bucket/
  artifacts:
    paths:
      - s3://my-test-bucket/
  script:
    - awslocal s3api list-buckets 
    - awslocal s3 ls
    - awslocal s3 mb s3://my-test-bucket
    - awslocal s3api list-buckets 
    - awslocal s3 cp ./ s3://my-test-bucket/ --recursive

dynamo-localstack:
  stage: dynamodb
  script:
    - echo '{"Artist":{"S":"Acme Band"},"SongTitle":{"S":"Happy Day"}, "AlbumTitle":{"S":"Songs About Life"},"Awards":{"N":"10"}}' > item_table.json 
    - echo '{"Artist":{"AttributeValueList":[{"S":"Acme Band"}],"ComparisonOperator":"EQ"},"SongTitle":{"AttributeValueList":[{"S":"Happy Day"}],"ComparisonOperator":"EQ"}}' > query_table.json 
    - awslocal configure set region $DEFAULT_REGION
    - awslocal dynamodb create-table --table-name Music --attribute-definitions AttributeName=Artist,AttributeType=S AttributeName=SongTitle,AttributeType=S --key-schema AttributeName=Artist,KeyType=HASH AttributeName=SongTitle,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1
    - awslocal dynamodb put-item --table-name Music --item  file://item_table.json  --return-consumed-capacity TOTAL 
    - awslocal dynamodb list-tables 
    - awslocal dynamodb query --table-name Music --key-conditions file://query_table.json
    



kafka-localstack:
  stage: kafka
  script:
    # create a virtual private cloud and save the vpc ID:
    - vpcID=$(awslocal ec2 create-vpc --cidr-block 10.0.0.0/16) 
    - echo $vpcID &&  echo "${vpcID}" > result.json
    - vpcID=$(jq -r '.Vpc.VpcId' result.json) && echo $vpcID

    # create a subrede to the vpc
    - subnetID=$(awslocal ec2 create-subnet --vpc-id $vpcID --cidr-block 10.0.1.0/24)
    - echo $subnetID &&  echo "${subnetID}" > result.json
    - subnetID=$(jq -r '.Subnet.SubnetId' result.json) && echo $subnetID

    # make the subnet public
    - InternetGateway=$(awslocal ec2 create-internet-gateway)
    - echo $InternetGateway &&  echo "${InternetGateway}" > result.json
    - InternetGateway=$(jq -r '.InternetGateway.InternetGatewayId' result.json) && echo $InternetGateway

    # attach the internet gateway to your VPC
    - awslocal ec2 attach-internet-gateway --vpc-id $vpcID --internet-gateway-id $InternetGateway
    - echo '{"BrokerNodeGroupInfo":{"BrokerAZDistribution":"DEFAULT","InstanceType":"kafka.m5.large","ClientSubnets":["'"${subnetID}"'"]},"ClusterName":"wortenCluster","EnhancedMonitoring":"PER_TOPIC_PER_BROKER","KafkaVersion":"2.2.1","NumberOfBrokerNodes":1}' >  kafka.json
    - cat kafka.json

    ### CREATE CLUSTERS
    - awslocal ec2 describe-vpcs --output table
    - awslocal ec2 describe-subnets --filters "Name=vpc-id,Values=$vpcID" --output table | egrep "Name|AvailabilityZone|SubnetId"
    - echo -e "auto.create.topics.enable = true\ndelete.topic.enable = true\nlog.retention.hours = 8" > cluster_config.txt
    - cat cluster_config.txt
    - awslocal kafka create-configuration --region us-west-2c --name "configuration" --description "configuration used for topic creation" --kafka-versions "2.3.1" "2.2.1" --server-properties file://cluster_config.txt 

    - result=$(awslocal kafka create-cluster --cli-input-json file://kafka.json ) && echo $result


