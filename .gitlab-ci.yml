image: ubuntu

stages:
  - lambda
  - dynamodb
  - create
  - test
  - list
  - golang
  - kafka

services:
  - name: localstack/localstack:latest
    alias: localstack

variables:
  SERVICES: s3,sns,sqs, dynamodb,lambda, kafka, ec2, iam
  HOSTNAME_EXTERNAL: localstack 
  AWS_DEFAULT_REGION: eu-west-2
  AWS_ACCESS_KEY_ID: localkey 
  AWS_SECRET_ACCESS_KEY: localsecret
  AWS_SESSION_TOKEN: 12345678
  AWS_DYNAMODB_ENDPOINT: http://dynamodb:8000
  LOCALSTACK_S3_BUCKET_NAME: lambda-bucket
  DATA_DIR: /tmp/localstack/data
  DOCKER_HOST: unix:///var/run/docker.sock
  LOCALSTACK_DEBUG: 1
  GIT_CLEAN_FLAGS: none
  LOCALSTACK_HOST: localstack
  LOCALSTACK_DEBUG: 1
  HOST: docker:2375
  #DOCKER_DRIVER: overlay2
  LS_LOG: trace
  #lambda env:
  LAMBDA_REMOTE_DOCKER: "true"
  LAMBDA_EXECUTOR: local
  HOST_TMP_FOLDER: $TMPDIR/localstack
  #BUCKET_MARKER_LOCAL: $LOCALSTACK_S3_BUCKET_NAME
  #HOSTNAME_FROM_LAMBDA: localstack

before_script:
 # - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.example.com
  - apt-get -y update
  - apt-get -y install python3-pip jq curl zip unzip
  - pip3 install --upgrade awscli  awscli-local
    
cache:
  paths:
    - s3.localhost.localstack.cloud/


Lambda.AWS.local:
  stage: lambda
  script:
     - bash ./src/sh/lambda.sh
     - awslocal lambda invoke --function-name arn:aws:lambda:eu-west-2:000000000000:function:hello out --region $AWS_DEFAULT_REGION --log-type Tail  --query 'LogResult' --output text |  base64 -d


test-localstack:
  stage: list
  dependencies:
    - test-application
  script:
    - awslocal s3 ls
    - awslocal s3api list-buckets 

test-application:
  stage: test 
  # extends: build
  # cache:
  #   paths:
  #     - s3://my-test-bucket/
  artifacts:
    paths:
      - s3://my-test-bucket/
  script:
    - awslocal s3api list-buckets 
    - awslocal s3 ls
    - awslocal s3 mb s3://my-test-bucket
    - awslocal s3api list-buckets 
    - awslocal s3 cp ./ s3://my-test-bucket/ --recursive

dynamo-localstack:
  stage: dynamodb
  script:
    - bash ./src/sh/dynamoDB.sh
    



kafka-localstack:
  stage: kafka
  script:
    # create a virtual private cloud and save the vpc ID:
    - vpcID=$(awslocal ec2 create-vpc --cidr-block 10.0.0.0/16) 
    - echo $vpcID &&  echo "${vpcID}" > result.json
    - vpcID=$(jq -r '.Vpc.VpcId' result.json) && echo $vpcID

    # create a subrede to the vpc
    - subnetID=$(awslocal ec2 create-subnet --vpc-id $vpcID --cidr-block 10.0.1.0/24)
    - echo $subnetID &&  echo "${subnetID}" > result.json
    - subnetID=$(jq -r '.Subnet.SubnetId' result.json) && echo $subnetID

    # make the subnet public
    - InternetGateway=$(awslocal ec2 create-internet-gateway)
    - echo $InternetGateway &&  echo "${InternetGateway}" > result.json
    - InternetGateway=$(jq -r '.InternetGateway.InternetGatewayId' result.json) && echo $InternetGateway

    # attach the internet gateway to your VPC
    - awslocal ec2 attach-internet-gateway --vpc-id $vpcID --internet-gateway-id $InternetGateway
    - echo '{"BrokerNodeGroupInfo":{"BrokerAZDistribution":"DEFAULT","InstanceType":"kafka.m5.large","ClientSubnets":["'"${subnetID}"'"]},"ClusterName":"wortenCluster","EnhancedMonitoring":"PER_TOPIC_PER_BROKER","KafkaVersion":"2.2.1","NumberOfBrokerNodes":1}' >  kafka.json
    - cat kafka.json

    ### CREATE CLUSTERS
    - awslocal ec2 describe-vpcs --output table
    - awslocal ec2 describe-subnets --filters "Name=vpc-id,Values=$vpcID" --output table | egrep "Name|AvailabilityZone|SubnetId"
    - echo -e "auto.create.topics.enable = true\ndelete.topic.enable = true\nlog.retention.hours = 8" > cluster_config.txt
    - cat cluster_config.txt
    - awslocal kafka create-configuration --region us-west-2c --name "configuration" --description "configuration used for topic creation" --kafka-versions "2.3.1" "2.2.1" --server-properties file://cluster_config.txt 

    - result=$(awslocal kafka create-cluster --cli-input-json file://kafka.json ) && echo $result


