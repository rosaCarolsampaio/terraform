image: ubuntu

stages:
  - lambda
  - kafka
  - golang
  - create
  - dynamodb
  - test
  - list

services:
  - name: localstack/localstack:latest
    alias: localstack

variables:
  SERVICES: s3,sns,sqs, dynamodb,lambda, kafka, ec2, iam
  HOSTNAME_EXTERNAL: localstack 
  AWS_DEFAULT_REGION: eu-west-2
  AWS_ACCESS_KEY_ID: localkey 
  AWS_SECRET_ACCESS_KEY: localsecret
  AWS_SESSION_TOKEN: 12345678
  AWS_DYNAMODB_ENDPOINT: http://dynamodb:8000
  LOCALSTACK_S3_BUCKET_NAME: lambda-bucket
  DATA_DIR: /tmp/localstack/data
  DOCKER_HOST: unix:///var/run/docker.sock
  LAMBDA_REMOTE_DOCKER: "FALSE"
  HOST_TMP_FOLDER: /tmp/
  LOCALSTACK_DEBUG: 1
  LAMBDA_EXECUTOR: docker
  GIT_CLEAN_FLAGS: none
  LOCALSTACK_HOST: localstack
  HOST: docker:2375
  #DOCKER_DRIVER: overlay2
  


before_script:
 # - docker login -u gitlab-ci-token -p $CI_BUILD_TOKEN registry.example.com
  - apt-get -y update
  - apt-get -y install python3-pip jq curl zip unzip
  - pip3 install --upgrade awscli  awscli-local
    
cache:
  paths:
    - s3.localhost.localstack.cloud/


Lambda.AWS.local:
  stage: lambda
  script:
    # copy the files and create trust:
    # - awslocal s3 mb s3://$LOCALSTACK_S3_BUCKET_NAME
     - awslocal s3api put-object --bucket $LOCALSTACK_S3_BUCKET_NAME --key lambda
     - awslocal s3 cp ./src/ s3://$LOCALSTACK_S3_BUCKET_NAME --recursive --acl public-read-write
     - curl -v http://localstack:4566/$LOCALSTACK_S3_BUCKET_NAME/index.js | true
    # create role:
     - awslocal iam create-role --role-name lambda-example --assume-role-policy-document file://src/script/trust-policy.json
    #attach the policy - a permission to role
     - awslocal iam attach-role-policy --role-name lambda-example --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
    #Create the function:
    # - awslocal lambda create-function --function-name hello --zip-file fileb://handler.zip --handler index.hello --runtime nodejs12.x --role arn:aws:iam::000000000000:role/lambda-example
     - zip -r function.zip src/index.js 
     - awslocal lambda create-function --function-name hello  --code S3Bucket=$LOCALSTACK_S3_BUCKET_NAME,S3Key=function.zip  --handler "index.handler"  --runtime "nodejs12.x"  --region "us-east-2" --memory-size "128"  --role arn:aws:iam::000000000000:role/lambda-example 
     - awslocal lambda update-function-code --function-name hello --zip-file fileb://function.zip
    # INVOKE:
    # - awslocal --endpoint-url $AWS_ENDPOINT lambda  invoke --function-name "hello" 
     - awslocal lambda list-functions --region us-east-2
     - awslocal lambda get-function  --region us-east-2 --function-name hello
     - awslocal lambda invoke --function-name  "arn:aws:lambda:us-east-2:000000000000:function:hello" out --log-type Tail --region us-west-2
     - awslocal lambda invoke --function-name "hello" out --log-type Tail --region us-east-2 --query 'LogResult' --output text |  base64 -d


test-localstack:
  stage: list
  dependencies:
    - test-application
  script:
    - awslocal s3 ls
    - awslocal s3api list-buckets 

test-application:
  stage: test 
  # extends: build
  # cache:
  #   paths:
  #     - s3://my-test-bucket/
  artifacts:
    paths:
      - s3://my-test-bucket/
  script:
    - awslocal s3api list-buckets 
    - awslocal s3 ls
    - awslocal s3 mb s3://my-test-bucket
    - awslocal s3api list-buckets 
    - awslocal s3 cp ./ s3://my-test-bucket/ --recursive

dynamo-localstack:
  stage: dynamodb
  script:
    - echo '{"Artist":{"S":"Acme Band"},"SongTitle":{"S":"Happy Day"}, "AlbumTitle":{"S":"Songs About Life"},"Awards":{"N":"10"}}' > item_table.json 
    - echo '{"Artist":{"AttributeValueList":[{"S":"Acme Band"}],"ComparisonOperator":"EQ"},"SongTitle":{"AttributeValueList":[{"S":"Happy Day"}],"ComparisonOperator":"EQ"}}' > query_table.json 
    - awslocal configure set region $DEFAULT_REGION
    - awslocal dynamodb create-table --table-name Music --attribute-definitions AttributeName=Artist,AttributeType=S AttributeName=SongTitle,AttributeType=S --key-schema AttributeName=Artist,KeyType=HASH AttributeName=SongTitle,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1
    - awslocal dynamodb put-item --table-name Music --item  file://item_table.json  --return-consumed-capacity TOTAL 
    - awslocal dynamodb list-tables 
    - awslocal dynamodb query --table-name Music --key-conditions file://query_table.json
    



kafka-localstack:
  stage: kafka
  script:
    # create a virtual private cloud and save the vpc ID:
    - vpcID=$(awslocal ec2 create-vpc --cidr-block 10.0.0.0/16) 
    - echo $vpcID &&  echo "${vpcID}" > result.json
    - vpcID=$(jq -r '.Vpc.VpcId' result.json) && echo $vpcID

    # create a subrede to the vpc
    - subnetID=$(awslocal ec2 create-subnet --vpc-id $vpcID --cidr-block 10.0.1.0/24)
    - echo $subnetID &&  echo "${subnetID}" > result.json
    - subnetID=$(jq -r '.Subnet.SubnetId' result.json) && echo $subnetID

    # make the subnet public
    - InternetGateway=$(awslocal ec2 create-internet-gateway)
    - echo $InternetGateway &&  echo "${InternetGateway}" > result.json
    - InternetGateway=$(jq -r '.InternetGateway.InternetGatewayId' result.json) && echo $InternetGateway

    # attach the internet gateway to your VPC
    - awslocal ec2 attach-internet-gateway --vpc-id $vpcID --internet-gateway-id $InternetGateway
    - echo '{"BrokerNodeGroupInfo":{"BrokerAZDistribution":"DEFAULT","InstanceType":"kafka.m5.large","ClientSubnets":["'"${subnetID}"'"]},"ClusterName":"wortenCluster","EnhancedMonitoring":"PER_TOPIC_PER_BROKER","KafkaVersion":"2.2.1","NumberOfBrokerNodes":1}' >  kafka.json
    - cat kafka.json

    ### CREATE CLUSTERS
    - awslocal ec2 describe-vpcs --output table
    - awslocal ec2 describe-subnets --filters "Name=vpc-id,Values=$vpcID" --output table | egrep "Name|AvailabilityZone|SubnetId"
    - echo -e "auto.create.topics.enable = true\ndelete.topic.enable = true\nlog.retention.hours = 8" > cluster_config.txt
    - cat cluster_config.txt
    - awslocal kafka create-configuration --region us-west-2c --name "configuration" --description "configuration used for topic creation" --kafka-versions "2.3.1" "2.2.1" --server-properties file://cluster_config.txt 

    - result=$(awslocal kafka create-cluster --cli-input-json file://kafka.json ) && echo $result


